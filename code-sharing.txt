package com.boa.training.sender;

import java.util.List;
import java.util.Properties;
import java.util.ArrayList;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

public class FirstSender {

	public static void main(String[] args) {
				
		Properties props=new Properties();
		props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
		props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
		
		KafkaProducer<String, String> producer=new KafkaProducer<String, String>(props);
		String topic="first-topic";
		
		
		for(int i=1;i<=10;i++){
			ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, "test-msg", "This is test message "+i);
			producer.send(record);
		}
		
		System.out.println("messages sent");
		producer.close();
	}

}

package com.boa.training.receiver;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

public class ReceiverTest1 {

	public static void main(String[] args) {
				
		Properties props=new Properties();
		props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "test-group");
		KafkaConsumer<String, String> consumer=new KafkaConsumer<String, String>(props);
		List<String> topics=new ArrayList<String>();
		topics.add("first-topic");
		consumer.subscribe(topics);
		System.out.println("waiting for messages");
		while(true){
			ConsumerRecords<String, String> records=consumer.poll(Duration.ofSeconds(5));
			for(ConsumerRecord<String, String> record:records){
				System.out.println("partition: "+record.partition()+" key: "+record.key()+" payload:"+ record.value());
			}
		}
		
	}

}

import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;





public class SenderWithCallback {

    public static void main(String[] args) {
                
        Properties props=new Properties();
        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        
        KafkaProducer<String, String> producer=new KafkaProducer<String, String>(props);
        String topic="first-topic";
        
        
        String key= "test-msg";
        MyCallback callback=new MyCallback(key);
        for(int i=1;i<=10;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is test message "+i);
            producer.send(record,callback);
        }
        key="next-msg";
        callback=new MyCallback(key);
        for(int i=11;i<=20;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is next message "+i);
            producer.send(record,callback);
        }
        key="third-msg";
        callback=new MyCallback(key);
        for(int i=21;i<=30;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is third message "+i);
            producer.send(record,callback);
        }
        System.out.println("messages sent");
        producer.close();
    }

}

package com.boa.training.sender.partitioner;

import java.util.Map;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;

public class MessagePartitioner implements Partitioner {

    @Override
    public void configure(Map<String, ?> map) {
        // TODO Auto-generated method stub
    System.out.println("configuring "); 
    }

    @Override
    public void close() {
        // TODO Auto-generated method stub
        System.out.println("closing the partitioner");
    }

    @Override
    public int partition(String topic, Object key, byte[] keyBytes, Object value,
            byte[] valueBytes, Cluster arg5) {
        // TODO Auto-generated method stub
        String msgKey=(String)key;
        int partition=3;
        if(msgKey.equals("first-msg")){
            partition=0;
        }
        else if(msgKey.equals("second-msg")){
            partition=1;
        }
        else if(msgKey.equals("third-msg")){
            partition=2;
        }
        
        return partition;
        @Override
    public void configure(Map<String, ?> map) {
        // TODO Auto-generated method stub
    System.out.println("configuring "); 
    }

    @Override
    public void close() {
        // TODO Auto-generated method stub
        System.out.println("closing the partitioner");
    }

    @Override
    public int partition(String topic, Object key, byte[] keyBytes, Object value,
            byte[] valueBytes, Cluster arg5) {
        // TODO Auto-generated method stub
        String msgKey=(String)key;
        int partition=3;
        if(msgKey.equals("first-msg")){
            partition=0;
        }
        else if(msgKey.equals("second-msg")){
            partition=1;
        }
        else if(msgKey.equals("third-msg")){
            partition=2;
        }
        
        return partition;
    }

}

package com.boa.training.sender;

import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

import com.boa.training.sender.partitioner.MessagePartitioner;





public class SenderWithCallbackAndPartitioner {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        
        Properties props=new Properties();
        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG, MessagePartitioner.class.getName());
        
        KafkaProducer<String, String> producer=new KafkaProducer<String, String>(props);
        String topic="first-topic";
        
        
        String key= "first-msg";
        MyCallback callback=new MyCallback(key);
        for(int i=1;i<=10;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is test message "+i);
            producer.send(record,callback);
        }
        key="second-msg";
        callback=new MyCallback(key);
        for(int i=11;i<=20;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is next message "+i);
            producer.send(record,callback);
        }
        key="third-msg";
        callback=new MyCallback(key);
        for(int i=21;i<=30;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is third message "+i);
            producer.send(record,callback);
        }
        
        key="fourth-msg";
        callback=new MyCallback(key);
        for(int i=31;i<=40;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is fourth message "+i);
            producer.send(record,callback);
        }
        
        key="fifth-msg";
        callback=new MyCallback(key);
        for(int i=41;i<=50;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is fifth message "+i);
            producer.send(record,callback);
        }
        System.out.println("messages sent");
        producer.close();
    }

}

package com.boa.training.receiver;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;



public class ReceiverFromSpecificPartitionTest {

    public static void main(String[] args) {
        Properties props=new Properties();
        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "first-group");
        KafkaConsumer<String, String> consumer=new KafkaConsumer<String, String>(props);
        List<TopicPartition> list=new ArrayList<TopicPartition>();
        TopicPartition topicPartition=new TopicPartition("first-topic", Integer.parseInt(args[0]));
        list.add(topicPartition);
        consumer.assign(list);
        System.out.println("waiting for messages");
        while(true){
            ConsumerRecords<String, String> records=consumer.poll(Duration.ofSeconds(5));
            for(ConsumerRecord<String, String> record:records){
                System.out.println("partition: "+record.partition()+" key: "+record.key()+" payload:"+ record.value());
            }
        }
        
    }

}

package com.boa.training.domain;

public class Account {
    private int accNo;
    private int customerId;
    private String accType;
    
    
    public int getAccNo() {
        return accNo;
    }
    public void setAccNo(int accNo) {
        this.accNo = accNo;
    }
    public int getCustomerId() {
        return customerId;
    }
    public void setCustomerId(int customerId) {
        this.customerId = customerId;
    }
    public String getAccType() {
        return accType;
    }
    public void setAccType(String accType) {
        this.accType = accType;
    }
    public Account(int accNo, int customerId, String accType) {
        super();
        this.accNo = accNo;
        this.customerId = customerId;
        this.accType = accType;
    }
    public Account() {
        super();
    }
    
}

package com.boa.training.domain;

public class Account {
    private int accNo;
    private int customerId;
    private String accType;
    
    
    public int getAccNo() {
        return accNo;
    }
    public void setAccNo(int accNo) {
        this.accNo = accNo;
    }
    public int getCustomerId() {
        return customerId;
    }
    public void setCustomerId(int customerId) {
        this.customerId = customerId;
    }
    public String getAccType() {
        return accType;
    }
    public void setAccType(String accType) {
        this.accType = accType;
    }
    public Account(int accNo, int customerId, String accType) {
        super();
        this.accNo = accNo;
        this.customerId = customerId;
        this.accType = accType;
    }
    public Account() {
        super();
        // TODO Auto-generated constructor stub
    }
    
    
}




package com.boa.training.serializer;

import org.apache.kafka.common.serialization.Serializer;

import com.boa.training.domain.Account;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;

public class AccountSerializer implements Serializer<Account>{

    private ObjectMapper mapper=new ObjectMapper();
    

    @Override
    public byte[] serialize(String topic, Account account) {
        // TODO Auto-generated method stub
        byte[] array=null;
        try {
            String jsonContent=mapper.writeValueAsString(account);
            System.out.println("serializing "+jsonContent);
            array=jsonContent.getBytes();
        } catch (JsonProcessingException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
        return array;
    }

}


package com.boa.training.sender;

import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

import com.boa.training.domain.Account;
import com.boa.training.serializer.AccountSerializer;

public class AccountSenderTest {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        
        Properties props=new Properties();
        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, AccountSerializer.class.getName());
        
        KafkaProducer<String, Account> producer=new KafkaProducer<String, Account>(props);
        int customerId=23444;
        for(int i=10001;i<=10010;i++){
            Account account=new Account(i, customerId, "SB");
            ProducerRecord<String, Account> record=new ProducerRecord<String, Account>("account-topic", "sb",account);
            producer.send(record);
            customerId++;
        }
        customerId=542322;
        for(int i=10011;i<=10020;i++){
            Account account=new Account(i, customerId, "Loan");
            ProducerRecord<String, Account> record=new ProducerRecord<String, Account>("account-topic", "loan",account);
            producer.send(record);
            customerId++;
        }

        
        customerId=982322;
        for(int i=10021;i<=10030;i++){
            Account account=new Account(i, customerId, "CA");
            ProducerRecord<String, Account> record=new ProducerRecord<String, Account>("account-topic", "ca",account);
            producer.send(record);
            customerId++;
        }
        System.out.println("messages sent");
        producer.close();
        
    }

}

package com.boa.training.deserializer;

import java.io.IOException;

import org.apache.kafka.common.serialization.Deserializer;

import com.boa.training.domain.Account;
import com.fasterxml.jackson.databind.ObjectMapper;

public class AccountDeserializer implements Deserializer<Account> {

	private ObjectMapper mapper=new ObjectMapper();

	@Override
	public Account deserialize(String topic, byte[] data) {
		Account account=null;
		System.out.println("Deserializing "+new String(data));
		try {
			account=mapper.readValue(data, Account.class);
		} catch (IOException e) {
			e.printStackTrace();
		}
		return account;
	}

}

package com.boa.training.receiver;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

import com.boa.training.deserializer.AccountDeserializer;
import com.boa.training.domain.Account;



public class AccountReceiverTest {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        
        Properties props=new Properties();
        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, AccountDeserializer.class.getName());
        props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "first-group");
        KafkaConsumer<String, Account> consumer=new KafkaConsumer<String, Account>(props);
        List<String> topics=new ArrayList<String>();
        topics.add("account-topic");
        consumer.subscribe(topics);
        System.out.println("waiting for messages");
        while(true){
            ConsumerRecords<String, Account> records=consumer.poll(Duration.ofSeconds(5));
            for(ConsumerRecord<String, Account> record:records){
                System.out.println("partition: "+record.partition()+" key: "+record.key());
                Account account=record.value();
                System.out.println("Account No: "+account.getAccNo());
                System.out.println("Customer Id: "+account.getCustomerId());
                System.out.println("Account Type: "+account.getAccType());
                
            }
        }
        
    }

}

package com.boa.training.paritioner;

import java.util.Map;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;

import com.boa.training.domain.Account;

public class AccountPartitioner implements Partitioner {

    @Override
    public void configure(Map<String, ?> arg0) {
        
    }

    @Override
    public void close() {
        
    }

    @Override
    public int partition(String topic, Object key, byte[] keyBytes, Object value,
            byte[] valueBytes, Cluster cluster) {
        Account account=(Account)value;
        int partition=4;
        if(account.getAccType().equals("SB")){
            partition=0;
        }
        else if(account.getAccType().equals("RD")){
            partition=1;
        }
        else if(account.getAccType().equals("CA")){
            partition=2;
        }
        else if(account.getAccType().equals("Loan")){
            partition=3;
        }
        return partition;
    }

}


package com.boa.training.receiver;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;

import com.boa.training.deserializer.AccountDeserializer;
import com.boa.training.domain.Account;

public class AccountReceiverFromSpecificPartitionTest {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        
        Properties props=new Properties();
        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, AccountDeserializer.class.getName());
        props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "first-group");
        //  props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
                
        KafkaConsumer<String, Account> consumer=new KafkaConsumer<String, Account>(props);
        List<TopicPartition> topicPartitions=new ArrayList<TopicPartition>();
        topicPartitions.add(new TopicPartition("account-topic", Integer.parseInt(args[0])));
        consumer.assign(topicPartitions);
        System.out.println("waiting for messages");
        while(true){
            ConsumerRecords<String, Account> records=consumer.poll(Duration.ofSeconds(5));
            for(ConsumerRecord<String, Account> record:records){
                System.out.println("partition: "+record.partition()+" key: "+record.key());
                Account account=record.value();
                System.out.println("Account No: "+account.getAccNo());
                System.out.println("Customer Id: "+account.getCustomerId());
                System.out.println("Account Type: "+account.getAccType());
                
            }
        }
        
    }

}


package com.boa.training.receiver;

import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;



public class ReceiverFromSpecificPartitionTest {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        
        Properties props=new Properties();
        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "first-group");
        KafkaConsumer<String, String> consumer=new KafkaConsumer<String, String>(props);
        List<TopicPartition> list=new ArrayList<TopicPartition>();
        TopicPartition topicPartition=new TopicPartition("first-topic", Integer.parseInt(args[0]));
        list.add(topicPartition);
        consumer.assign(list);
        consumer.seek(topicPartition, 0);
        System.out.println("waiting for messages");
        while(true){
            ConsumerRecords<String, String> records=consumer.poll(Duration.ofSeconds(5));
            for(ConsumerRecord<String, String> record:records){
                System.out.println("partition: "+record.partition()+" offset: "+record.offset()+" key: "+record.key()+" payload:"+ record.value());
                
            }
        }
        
    }

}

package com.boa.training.sender;

import java.util.Properties;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.serialization.StringSerializer;





public class SynchronousSender {

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        
        Properties props=new Properties();
        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        
        KafkaProducer<String, String> producer=new KafkaProducer<String, String>(props);
        String topic="first-topic";
        
        
        String key= "test-msg";
        
        for(int i=1;i<=10;i++){
            ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, key, "This is test message "+i);
            System.out.println("sending the message "+i);
            Future<RecordMetadata> future=producer.send(record);
            try {
                RecordMetadata rmd=future.get();
                System.out.println("message published to "+rmd.partition()+" at offset "+rmd.offset());
            } catch (InterruptedException | ExecutionException e) {
                e.printStackTrace();
            }
            //producer.send(record, new MyCallback(key));
            
        }
        
        producer.close();
    }

}

package com.boa.training.sender;

import java.io.File;
import java.util.List;
import java.util.Properties;

import javax.xml.bind.JAXBContext;
import javax.xml.bind.JAXBException;
import javax.xml.bind.Unmarshaller;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

import com.boa.training.domain.Account;
import com.boa.training.paritioner.AccountPartitioner;
import com.boa.training.serializer.AccountSerializer;

public class AccountXMLSenderTest {

	public static void main(String[] args) {
				
		Properties props=new Properties();
		props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
		props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, AccountSerializer.class.getName());
		props.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG, AccountPartitioner.class.getName());
		
		
		KafkaProducer<String, Account> producer=new KafkaProducer<String, Account>(props);
		Account account=readFromFile();
		ProducerRecord<String, Account> record=new ProducerRecord<String, Account>("account-topic", "", account);
		producer.send(record);
		System.out.println("message sent");
		producer.close();
		
	}
	
	static Account readFromFile()
	{
		Account account=null;
		try {
			JAXBContext context=JAXBContext.newInstance(Account.class);
			Unmarshaller unmarshaller=context.createUnmarshaller();
			account=(Account) unmarshaller.unmarshal(new File("account.xml"));
		} catch (JAXBException e) {
			e.printStackTrace();
		}
		return account;
		
	}

}


package com.boa.training.sender;

import java.io.File;
import java.util.List;
import java.util.Properties;

import javax.xml.bind.JAXBContext;
import javax.xml.bind.JAXBException;
import javax.xml.bind.Unmarshaller;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

import com.boa.training.domain.Account;
import com.boa.training.paritioner.AccountPartitioner;
import com.boa.training.serializer.AccountSerializer;

public class AccountXMLSenderTest {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		
		Properties props=new Properties();
		props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
		props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, AccountSerializer.class.getName());
		props.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG, AccountPartitioner.class.getName());
		
		
		KafkaProducer<String, Account> producer=new KafkaProducer<String, Account>(props);
		Account account=readFromFile();
		ProducerRecord<String, Account> record=new ProducerRecord<String, Account>("account-topic", "", account);
		producer.send(record);
		System.out.println("message sent");
		producer.close();
		
	}
	
	static Account readFromFile()
	{
		Account account=null;
		try {
			JAXBContext context=JAXBContext.newInstance(Account.class);
			Unmarshaller unmarshaller=context.createUnmarshaller();
			account=(Account) unmarshaller.unmarshal(new File("account.xml"));
		} catch (JAXBException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		return account;
		
	}

}

source-connector-mysql.properties

#
# Copyright 2018 Confluent Inc.
#
# Licensed under the Confluent Community License; you may not use this file
# except in compliance with the License.  You may obtain a copy of the License at
#
# http://www.confluent.io/confluent-community-license
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OF ANY KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations under the License.
#

# A simple example that copies all tables from a SQLite database. The first few settings are
# required for all connectors: a name, the connector class to run, and the maximum number of
# tasks to create:
name=test-source-mysql-jdbc-autoincrement
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=1
# The remaining configs are specific to the JDBC source connector. In this example, we connect to a
# SQLite database stored in the file test.db, use and auto-incrementing column called 'id' to
# detect new rows as they are added, and output to topics prefixed with 'test-sqlite-jdbc-', e.g.
# a table called 'users' will be written to the topic 'test-sqlite-jdbc-users'.
#connection.url=jdbc:mysql://hostname:3306/boakafkatrainingdb
connection.url=jdbc:mysql://192.168.20.153:3306/trainingdb
connection.user=root
connection.password=root
mode=incrementing
incrementing.column.name=account_no
table.whitelist=account_tbl
topic.prefix=topic-test-mysql-
table.whitelist=account_tbl



#
# Copyright 2018 Confluent Inc.
#
# Licensed under the Confluent Community License; you may not use this file
# except in compliance with the License.  You may obtain a copy of the License at
#
# http://www.confluent.io/confluent-community-license
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OF ANY KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations under the License.
#

# A simple example that copies all tables from a SQLite database. The first few settings are
# required for all connectors: a name, the connector class to run, and the maximum number of
# tasks to create:
name=test-source-mysql-jdbc-autoincrement
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=1
# The remaining configs are specific to the JDBC source connector. In this example, we connect to a
# SQLite database stored in the file test.db, use and auto-incrementing column called 'id' to
# detect new rows as they are added, and output to topics prefixed with 'test-sqlite-jdbc-', e.g.
# a table called 'users' will be written to the topic 'test-sqlite-jdbc-users'.
connection.url=jdbc:mysql://hostname:3306/boakafkatrainingdb
connection.user=root
connection.password=root
mode=incrementing
incrementing.column.name=account_no
table.whitelist=account_tbl
topic.prefix=topic-test-mysql-
table.whitelist=account_tbl

import java.io.IOException;

import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serializer;

import com.boa.training.deserializer.AccountDeserializer;
import com.boa.training.domain.Account;
import com.boa.training.serializer.AccountSerializer;

public class AccountSerde  implements Serde<Account>{

	@Override
	public Deserializer<Account> deserializer() {
		return new AccountDeserializer();
	}

	@Override
	public Serializer<Account> serializer() {
		return new AccountSerializer();
	}

}

package com.boa.training.streams;

import java.util.Properties;

import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes.StringSerde;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KStream;

import com.boa.training.domain.Account;
import com.boa.training.serde.AccountSerde;

public class StreamingApp {
	public static void main(String[] args) {
		
		Properties props=new Properties();
		props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "account_transform_interest_rate_app");
		props.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,StringSerde.class.getName());
		props.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,AccountSerde.class.getName());
		props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		
		StreamsBuilder builder=new StreamsBuilder();
		KStream<String, Account> srcStream=builder.stream("topic-mysql-apache-kafka-account_tbl");
		KStream<String, Account> targetStream=srcStream.mapValues(account->{
			Account acc=new Account();
			acc.setAccNo(account.getAccNo());
			acc.setCustomerId(account.getCustomerId());
			acc.setAccType(account.getAccType());
			acc.setBalance(account.getBalance());
			double interestRate=0;
			if(account.getAccType().equals("SB")){
				interestRate=5.2;
			}
			else if(account.getAccType().equals("RD")){
				interestRate=7.4;
			}
			if(account.getAccType().equals("CA")){
				interestRate=0;
			}
			acc.setInterestRate(interestRate);
			return acc;
		});
		targetStream.to("topic-interest-rate");
		KafkaStreams streams=new KafkaStreams(builder.build(), props);
		streams.start();
		
	}

}



@XmlRootElement
public class Account {
	private int account_no;
	private int customer_id;
	private String acc_type;
	private double balance;
	private double interestRate;
	public int getAccount_no() {
		return account_no;
	}
	public void setAccount_no(int account_no) {
		this.account_no = account_no;
	}
	public int getCustomer_id() {
		return customer_id;
	}
	public void setCustomer_id(int customer_id) {
		this.customer_id = customer_id;
	}
	public String getAcc_type() {
		return acc_type;
	}
	public void setAcc_type(String acc_type) {
		this.acc_type = acc_type;
	}
	public double getBalance() {
		return balance;
	}
	public void setBalance(double balance) {
		this.balance = balance;
	}
	public double getInterestRate() {
		return interestRate;
	}
	public void setInterestRate(double interestRate) {
		this.interestRate = interestRate;
	}
	public Account() {
		super();
		// TODO Auto-generated constructor stub
	}
	public Account(int account_no, int customer_id, String acc_type,
			double balance, double interestRate) {
		super();
		this.account_no = account_no;
		this.customer_id = customer_id;
		this.acc_type = acc_type;
		this.balance = balance;
		this.interestRate = interestRate;
	}

}

package com.boa.training.streams;

import java.util.Properties;

import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes.StringSerde;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KStream;

import com.boa.training.domain.Account;
import com.boa.training.serde.AccountSerde;

public class StreamingApp {
	public static void main(String[] args) {
		
		Properties props=new Properties();
		props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "account_transform_interest_rate_app");
		props.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,StringSerde.class.getName());
		props.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,AccountSerde.class.getName());
		props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		
		StreamsBuilder builder=new StreamsBuilder();
		KStream<String, Account> srcStream=builder.stream("topic-mysql-apache-kafka-account_tbl");
		KStream<String, Account> targetStream=srcStream.mapValues(account->{
			Account acc=new Account();
			acc.setAccount_no(account.getAccount_no());
			acc.setCustomer_id(account.getCustomer_id());
			acc.setAcc_type(account.getAcc_type());
			acc.setBalance(account.getBalance());
			double interestRate=0;
			if(account.getAcc_type().equals("SB")){
				interestRate=5.2;
			}
			else if(account.getAcc_type().equals("RD")){
				interestRate=7.4;
			}
			if(account.getAcc_type().equals("CA")){
				interestRate=0;
			}
			acc.setInterestRate(interestRate);
			return acc;
		});
		targetStream.to("topic-interest-rate");
		KafkaStreams streams=new KafkaStreams(builder.build(), props);
		streams.start();
		
	}

}

Ubuntu Pastebin
Paste from test at Thu, 28 Nov 2019 09:49:00 +0000
Download as text
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
package com.boa.training.streams;

import java.util.Properties;

import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes.StringSerde;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KStream;

import com.boa.training.domain.Account;
import com.boa.training.serde.AccountSerde;

public class StreamingApp {
	public static void main(String[] args) {
		
		Properties props=new Properties();
		props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "account_transform_interest_rate_app");
		props.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,StringSerde.class.getName());
		props.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,AccountSerde.class.getName());
		props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		
		StreamsBuilder builder=new StreamsBuilder();
		KStream<String, Account> srcStream=builder.stream("topic-mysql-apache-kafka-account_tbl");
		KStream<String, Account> targetStream=srcStream.mapValues(account->{
			Account acc=new Account();
			acc.setAccount_no(account.getAccount_no());
			acc.setCustomer_id(account.getCustomer_id());
			acc.setAcc_type(account.getAcc_type());
			acc.setBalance(account.getBalance());
			double interestRate=0;
			if(account.getAcc_type().equals("SB")){
				interestRate=5.2;
			}
			else if(account.getAcc_type().equals("RD")){
				interestRate=7.4;
			}
			if(account.getAcc_type().equals("CA")){
				interestRate=0;
			}
			acc.setInterestRate(interestRate);
			return acc;
		});
		targetStream.to("topic-interest-rate");
		KafkaStreams streams=new KafkaStreams(builder.build(), props);
		streams.start();
		
	}

}


package com.boa.training.streams;

import java.util.Properties;

import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes.StringSerde;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Produced;

import com.boa.training.domain.Account;
import com.boa.training.serde.AccountSerde;

public class StreamingCSVApp {
	public static void main(String[] args) {
		
		Properties props=new Properties();
		props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "account_transform_interest_rate_app");
		props.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,StringSerde.class.getName());
		props.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,AccountSerde.class.getName());
		props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		
		StreamsBuilder builder=new StreamsBuilder();
		KStream<String, Account> srcStream=builder.stream("topic-mysql-apache-kafka-account_tbl");
		KStream<String, String> targetStream=srcStream.mapValues(account->{
			Account acc=new Account();
			acc.setAccount_no(account.getAccount_no());
			acc.setCustomer_id(account.getCustomer_id());
			acc.setAcc_type(account.getAcc_type());
			acc.setBalance(account.getBalance());
			double interestRate=0;
			if(account.getAcc_type().equals("SB")){
				interestRate=5.2;
			}
			else if(account.getAcc_type().equals("RD")){
				interestRate=7.4;
			}
			if(account.getAcc_type().equals("CA")){
				interestRate=0;
			}
			acc.setInterestRate(interestRate);
			return acc.getAccount_no()+","+acc.getCustomer_id()+","+acc.getBalance()+","+acc.getInterestRate();
		});
		targetStream.to("topic-interest-rate-csv" ,Produced.valueSerde(new StringSerde()));
		KafkaStreams streams=new KafkaStreams(builder.build(), props);
		streams.start();
		
	}

}


@echo off
bin\windows\kafka-server-start.bat config\server-1.properties


package com.boa.training.streams;

import java.util.Properties;

import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes.StringSerde;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Produced;

import com.boa.training.domain.Account;
import com.boa.training.serde.AccountSerde;

public class StreamingCSVApp {
	public static void main(String[] args) {
		
		Properties props=new Properties();
		props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "account_transform_interest_rate_app");
		props.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,StringSerde.class.getName());
		props.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,AccountSerde.class.getName());
		props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		
		StreamsBuilder builder=new StreamsBuilder();
		KStream<String, Account> srcStream=builder.stream("topic-mysql-apache-kafka-account_tbl");
		KStream<String, String> targetStream=srcStream.mapValues(account->{
			Account acc=new Account();
			acc.setAccount_no(account.getAccount_no());
			acc.setCustomer_id(account.getCustomer_id());
			acc.setAcc_type(account.getAcc_type());
			acc.setBalance(account.getBalance());
			double interestRate=0;
			if(account.getAcc_type().equals("SB")){
				interestRate=5.2;
			}
			else if(account.getAcc_type().equals("RD")){
				interestRate=7.4;
			}
			if(account.getAcc_type().equals("CA")){
				interestRate=0;
			}
			acc.setInterestRate(interestRate);
			return "\""+acc.getAccount_no()+","+acc.getCustomer_id()+","+acc.getBalance()+","+acc.getInterestRate()+"\"";
		});
		targetStream.to("topic-interest-rate-csv" ,Produced.valueSerde(new StringSerde()));
		KafkaStreams streams=new KafkaStreams(builder.build(), props);
		streams.start();
		
	}

}

server-secure.properties

listeners=SASL_PLAINTEXT://localhost:9092
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN

server-jaas-config.properties

KafkaServer {
   org.apache.kafka.common.security.plain.PlainLoginModule required
   username="admin"
   password="admin-secret"
   user_admin="admin-secret"
   user_alice="alice-secret";
};

TO ENABLE SASL SECURITY

set KAFKA_OPTS=-Djava.security.auth.login.config=config/server-jaas-config.properties

D:\rajesh\kafka_2.12-2.3.1>set KAFKA_OPTS=-Djava.security.auth.login.config=config/server-jaas-config.properties

D:\rajesh\kafka_2.12-2.3.1>bin\windows\kafka-server-start.bat config\server-secure.properties

client-jaas-config.properties

KafkaClient {
  org.apache.kafka.common.security.plain.PlainLoginModule required
  username="alice"
  password="alice-secret";
};

package com.boa.training.sender;

import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

public class SecureSender {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		
		Properties props=new Properties();
		props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
		props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
		props.setProperty("security.protocol", "SASL_PLAINTEXT");
		props.setProperty("sasl.mechanism","PLAIN");
		
		
		KafkaProducer<String, String> producer=new KafkaProducer<String, String>(props);
		String topic="first-topic";
		
		
		for(int i=1;i<=10;i++){
			ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, "test-msg", "This is test message "+i);
			producer.send(record);
		}
		
		for(int i=11;i<=20;i++){
			ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, "next-msg", "This is next message "+i);
			producer.send(record);
		}
		
		for(int i=21;i<=30;i++){
			ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, "third-msg", "This is third message "+i);
			producer.send(record);
		}
		System.out.println("messages sent");
		producer.close();
	}

}

<repositories>
   <repository>
            <id>confluent</id>
            <url>http://packages.confluent.io/maven/</url>
        </repository>
  </repositories>
  <dependencies>
  <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-avro-serializer</artifactId>
            <version>3.1.1</version>
        </dependency>
        
        <dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.12</artifactId>
    <version>2.1.1</version>
</dependency>
  </dependencies>

public class Customer {
	private int id;
	private String name;
	private String email;
	public Customer(int id, String name, String email) {
		super();
		this.id = id;
		this.name = name;
		this.email = email;
	}
	public Customer() {
		super();
		// TODO Auto-generated constructor stub
	}
	public int getId() {
		return id;
	}
	public void setId(int id) {
		this.id = id;
	}
	public String getName() {
		return name;
	}
	public void setName(String name) {
		this.name = name;
	}
	public String getEmail() {
		return email;
	}
	public void setEmail(String email) {
		this.email = email;
	}

}

package com.boa.training.avro;

import java.util.Properties;

import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;


public class Sender {
public static void main(String[] args) {
	Properties props = new Properties();
	props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
	props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
	"io.confluent.kafka.serializers.KafkaAvroSerializer");
	props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
	"io.confluent.kafka.serializers.KafkaAvroSerializer");
	props.put("schema.registry.url", "http://localhost:8081");
	String schemaString = "{\"namespace\": \"customerManagement.avro\","+
	"\"type\": \"record\", " +
	"\"name\": \"Customer\"," +
	"\"fields\": [" +
	"{\"name\": \"id\", \"type\": \"int\"}," +
	"{\"name\": \"name\", \"type\": \"string\"}," +
	"{\"name\": \"email\", \"type\": \"string\" }" +
	"]}";
	Producer<String, GenericRecord> producer =
	new KafkaProducer<String, GenericRecord>(props);
	Schema.Parser parser = new Schema.Parser();
	Schema schema = parser.parse(schemaString);
	for (int nCustomers = 0; nCustomers < 5; nCustomers++) {
	String name = "exampleCustomer" + nCustomers;
	String email = "example " + nCustomers + "@example.com";
	GenericRecord customer = new GenericData.Record(schema);
	customer.put("id", nCustomers);
	customer.put("name", name);
	customer.put("email", email);
	ProducerRecord<String, GenericRecord> data =
	new ProducerRecord<String,
	GenericRecord>("customerContacts",	name, customer);
	producer.send(data);
}
	producer.close();
}
}

import java.io.IOException;
import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

import org.apache.avro.generic.GenericRecord;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.ObjectMapper;


public class Receiver {
	public static void main(String[] args) throws JsonParseException, JsonMappingException, IOException {
		
			Properties props = new Properties();
			props.put("bootstrap.servers", "localhost:9092");
			props.put("group.id", "CountryCounter");
			props.put("key.deserializer",
			"org.apache.kafka.common.serialization.StringDeserializer");
			props.put("value.deserializer",
			"io.confluent.kafka.serializers.KafkaAvroDeserializer");
			props.put("schema.registry.url", "http://localhost:8081");
			String topic = "customerContacts";
			KafkaConsumer consumer = new
			KafkaConsumer(props);
			consumer.subscribe(Collections.singletonList(topic));
			System.out.println("Reading topic:" + topic);
			while (true) {
			ConsumerRecords<String, GenericRecord> records =
			consumer.poll(Duration.ofMinutes(2));
			for (ConsumerRecord<String, GenericRecord> record: records) {
				ObjectMapper mapper=new ObjectMapper();
				Customer c=mapper.readValue(record.value().toString().getBytes(), Customer.class);
			System.out.println("Current customer name is: " +
			c.getName());
			}
			
			}
	}

}