JAVA_HOME

C:\ProgramData\Oracle\Java\javapath

to start the zookeeper server

D:\rajesh\kafka_2.12-2.3.1>bin\windows\zookeeper-server-start.bat config\zookeeper.properties

to start kafka server

D:\rajesh\kafka_2.12-2.3.1>bin\windows\kafka-server-start.bat config\server.properties

D:\rajesh\kafka_2.12-2.3.1>bin\windows\kafka-server-start.bat config\server-ssl.properties

to create topic

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-topics.bat --create --topic first-topic --partitions 5 --replication-factor 1 --zookeeper localhost:2181

kafka-topics.bat --create --topic kafka-ssl-topic --partitions 5 --replication-factor 1 --zookeeper localhost:2181


Useful Commands

List Topics

kafka-topics.bat --list --zookeeper localhost:2181

Create Topic

kafka-topics.bat --create --topic first-topic --partitions 4 --replication-factor 3 --zookeeper localhost:2181

kafka-topics.bat --create --topic account-topic --partitions 5 --replication-factor 1 --zookeeper localhost:2181

Describe Topic

kafka-topics.bat --describe --zookeeper localhost:2181 --topic [Topic Name]

kafka-topics.bat --describe --zookeeper localhost:2181 --topic first-topic

kafka-topics.bat --alter --topic account-topic --partitions 5 --zookeeper localhost:2181

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-topics.bat --create --topic account-topic --partitions 5 --replication-factor 1 --zookeeper localhost:2181
Created topic account-topic.

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-topics.bat --list --zookeeper localhost:2181
__consumer_offsets
account-topic
first-topic

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-topics.bat --describe --zookeeper localhost:2181 --topic account-topic

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-topics.bat --describe --zookeeper localhost:2181 --topic first-topic

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-console-producer.bat --topic first-topic --broker-list localhost:9092
>Hello World!!!

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-console-consumer.bat --topic first-topic --bootstrap-server localhost:9092
Hello World!!!

kafka-console-consumer.bat --topic first-topic --bootstrap-server localhost:9092

kafka-console-producer.bat --topic account-topic --broker-list localhost:9092

kafka-console-consumer.bat --topic account-topic --bootstrap-server localhost:9092

Read messages from the beginning: 

kafka-console-consumer.bat --zookeeper localhost:2181 --topic [Topic Name] --from-beginning

kafka-console-consumer.bat --zookeeper localhost:2181 --topic first-topic --from-beginning

Delete Topic: 

kafka-run-class.bat kafka.admin.TopicCommand --delete --topic [topic_to_delete] --zookeeper localhost:2181

kafka-run-class.bat kafka.admin.TopicCommand --delete --topic first-topic --zookeeper localhost:2181

kafka-console-producer.bat --topic first-topic --broker-list localhost:9092

kafka-console-consumer.bat --group group-1 --topic first-topic --property print.key=true --bootstrap-server localhost:9092

kafka-console-consumer.bat --topic first-topic --bootstrap-server localhost:9092 --partition 0

kafka-console-consumer.bat --group test-group --topic first-topic --bootstrap-server localhost:9092 --property print.key=true

kafka-console-consumer.bat --topic kafka-ssl-topic --bootstrap-server localhost:9093 --consumer.config D:\rajesh\kafka-ssl-client\kafka-client.properties --from-beginning

kafka-console-producer.bat --topic kafka-ssl-topic --broker-list localhost:9093 --producer.config D:\rajesh\kafka-ssl-client\kafka-client.properties

file 

D:\rajesh\kafka_2.12-2.3.1>bin\windows\connect-standalone.bat config\connect-standalone.properties config\connect-file-source.properties

bin\windows\connect-standalone.bat config\connect-standalone.properties config\connect-file-source.properties

file to file sink properties

bin\windows\connect-standalone.bat config\connect-standalone.properties config\connect-file-source.properties config\connect-file-sink.properties

mysql to file sink properties

bin\windows\connect-standalone.bat config\connect-standalone.properties config\source-connector-mysql.properties config\connect-file-sink.properties

kafka-console-consumer.bat --topic topic-file-connect --bootstrap-server localhost:9092

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-console-consumer.bat --topic topic-file-connect --bootstrap-server localhost:9092
Processed a total of 0 messages
Terminate batch job (Y/N)? y

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-console-consumer.bat --topic topic-file-connect --bootstrap-server localhost:9092 --from-beginning
{"schema":{"type":"string","optional":false},"payload":"this is a test file "}
{"schema":{"type":"string","optional":false},"payload":"thank you"}
{"schema":{"type":"string","optional":false},"payload":"Hllow"}
{"schema":{"type":"string","optional":false},"payload":""}
{"schema":{"type":"string","optional":false},"payload":"htiahh"}
{"schema":{"type":"string","optional":false},"payload":""}

https://paste.ubuntu.com/p/qdKjWK5XZy/

6th generation

windows@123

We have Received your Get SMS request for PNR Number : 6137426162 You will shortly receive Booking SMS on your mobile number : 7993911050

We have Received your Get SMS request for PNR Number : 6137426162 You will shortly receive Booking SMS on your mobile number : 

7993911050

D:\rajesh\kafka_2.12-2.3.1\bin\windows>kafka-console-consumer.bat --help
This tool helps to read data from Kafka topics and outputs it to standard output.
Option                                   Description
------                                   -----------
--bootstrap-server <String: server to    REQUIRED: The server(s) to connect to.
  connect to>
--consumer-property <String:             A mechanism to pass user-defined
  consumer_prop>                           properties in the form key=value to
                                           the consumer.
--consumer.config <String: config file>  Consumer config properties file. Note
                                           that [consumer-property] takes
                                           precedence over this config.
--enable-systest-events                  Log lifecycle events of the consumer
                                           in addition to logging consumed
                                           messages. (This is specific for
                                           system tests.)
--formatter <String: class>              The name of a class to use for
                                           formatting kafka messages for
                                           display. (default: kafka.tools.
                                           DefaultMessageFormatter)
--from-beginning                         If the consumer does not already have
                                           an established offset to consume
                                           from, start with the earliest
                                           message present in the log rather
                                           than the latest message.
--group <String: consumer group id>      The consumer group id of the consumer.
--help                                   Print usage information.
--isolation-level <String>               Set to read_committed in order to
                                           filter out transactional messages
                                           which are not committed. Set to
                                           read_uncommittedto read all
                                           messages. (default: read_uncommitted)
--key-deserializer <String:
  deserializer for key>
--max-messages <Integer: num_messages>   The maximum number of messages to
                                           consume before exiting. If not set,
                                           consumption is continual.
--offset <String: consume offset>        The offset id to consume from (a non-
                                           negative number), or 'earliest'
                                           which means from beginning, or
                                           'latest' which means from end
                                           (default: latest)
--partition <Integer: partition>         The partition to consume from.
                                           Consumption starts from the end of
                                           the partition unless '--offset' is
                                           specified.
--property <String: prop>                The properties to initialize the
                                           message formatter. Default
                                           properties include:
        print.
                                           timestamp=true|false
        print.
                                           key=true|false
        print.
                                           value=true|false
        key.separator=<key.
                                           separator>
        line.separator=<line.
                                           separator>
        key.deserializer=<key.
                                           deserializer>
        value.
                                           deserializer=<value.deserializer>
                                           Users can also pass in customized
                                           properties for their formatter; more
                                           specifically, users can pass in
                                           properties keyed with 'key.
                                           deserializer.' and 'value.
                                           deserializer.' prefixes to configure
                                           their deserializers.
--skip-message-on-error                  If there is an error when processing a
                                           message, skip it instead of halt.
--timeout-ms <Integer: timeout_ms>       If specified, exit if no message is
                                           available for consumption for the
                                           specified interval.
--topic <String: topic>                  The topic id to consume on.
--value-deserializer <String:
  deserializer for values>
--version                                Display Kafka version.
--whitelist <String: whitelist>          Regular expression specifying
                                           whitelist of topics to include for
                                           consumption.
										   
										   

\\192.168.20.93\oracle

D:/rajesh/kafka_2.12-2.3.1/

root

mysql> desc account;
+--------------+---------------+------+-----+---------+----------------+
| Field        | Type          | Null | Key | Default | Extra          |
+--------------+---------------+------+-----+---------+----------------+
| account_no   | int(11)       | NO   | PRI | NULL    | auto_increment |
| customer_id  | int(11)       | YES  |     | NULL    |                |
| account_type | varchar(25)   | YES  |     | NULL    |                |
| balance      | decimal(10,2) | YES  |     | NULL    |                |
+--------------+---------------+------+-----+---------+----------------+
4 rows in set (0.01 sec)

INSERT INTO account (account_no, customer_id, account_type, balance) VALUES (100001, 12345, "SB", 145689.10);
INSERT INTO account (customer_id, account_type, balance) VALUES (12346, "LA", 155689.10);
INSERT INTO account (customer_id, account_type, balance) VALUES (12347, "CA", 165689.10);
INSERT INTO account (customer_id, account_type, balance) VALUES (12348, "RD", 175689.10);

connection.url=jdbc:mysql://hostname:3306/boakafkatrainingdb?user=root&password=root
mode=incrementing
incrementing.column.name=account_no
topic.prefix=topic-mysql-
table.whitelist=account_tbl

VM

drwxrwxrwx.  4 root  root    75 Nov 27 16:13 Kafka
drwx------. 19 kafka kafka 4096 Nov 27 16:18 ..
[kafka@localhost Desktop]$ cd ..
[kafka@localhost ~]$ cd ~/Desktop/
Kafka/            Old Firefox Data/ 
[kafka@localhost ~]$ cd ~/Desktop/
Kafka/            Old Firefox Data/ 
[kafka@localhost ~]$ cd ~/Desktop/
[kafka@localhost Desktop]$ ls -ltra
total 4
drwxr-xr-x.  4 kafka kafka   43 Nov 24 12:12 .
drwx------.  3 kafka kafka   30 Nov 24 12:12 Old Firefox Data
drwxrwxrwx.  4 root  root    75 Nov 27 16:13 Kafka
drwx------. 19 kafka kafka 4096 Nov 27 16:18 ..
[kafka@localhost Desktop]$ cd Kafka/
[kafka@localhost Kafka]$ ls -ltra
total 778784
drwxr-xr-x. 7 kafka kafka        77 Sep  5 01:02 confluent-5.3.1
drwxr-xr-x. 4 kafka kafka        43 Nov 24 12:12 ..
-rw-------. 1 kafka kafka 797471754 Nov 24 12:44 confluent-5.3.1-2.12.tar.gz
drwxrwxrwx. 4 root  root         75 Nov 27 16:13 .
drwxrwxrwx. 3 root  root         64 Nov 27 16:16 Old
[kafka@localhost Kafka]$ cd Old/confluent-5.1.0/
[kafka@localhost confluent-5.1.0]$ ls -ltra
total 12
drwxr-xr-x.  3 kafka kafka   21 Dec 15  2018 lib
drwxr-xr-x.  7 kafka kafka  106 Dec 15  2018 share
drwxr-xr-x. 23 kafka kafka 4096 Dec 15  2018 etc
drwxr-xr-x.  3 kafka kafka 4096 Dec 15  2018 bin
drwxr-xr-x.  2 kafka kafka  179 Dec 15  2018 src
-rw-r--r--.  1 kafka kafka  871 Dec 15  2018 README
drwxr-xr-x.  7 kafka kafka   77 Dec 15  2018 .
drwxrwxrwx.  3 root  root    64 Nov 27 16:16 ..
[kafka@localhost confluent-5.1.0]$ clear

[kafka@localhost confluent-5.1.0]$ ls 
bin  etc  lib  README  share  src
[kafka@localhost confluent-5.1.0]$ pwd
/home/kafka/Desktop/Kafka/Old/confluent-5.1.0
[kafka@localhost confluent-5.1.0]$ 
[kafka@localhost confluent-5.1.0]$ cd bin/
[kafka@localhost bin]$ sudo ./confluent start
[sudo] password for kafka: 
This CLI is intended for development only, not for production
https://docs.confluent.io/current/cli/index.html

Using CONFLUENT_CURRENT: /tmp/confluent.PN3Stqv5
Starting zookeeper
zookeeper is [UP]
Starting kafka
kafka is [UP]
Starting schema-registry
schema-registry is [UP]
Starting kafka-rest
kafka-rest is [UP]
Starting connect
connect is [UP]
Starting ksql-server
ksql-server is [UP]
Starting control-center
control-center is [UP]
[kafka@localhost bin]$ pwd
/home/kafka/Desktop/Kafka/Old/confluent-5.1.0/bin

[kafka@localhost kafka-connect-jdbc]$ pwd
/home/kafka/Desktop/Kafka/Old/confluent-5.1.0/etc/kafka-connect-jdbc

start all the services

[kafka@localhost bin]$ sudo ./confluent start

stop all the services

[kafka@localhost bin]$ sudo ./confluent stop

start a specific service

[kafka@localhost bin]$ sudo ./confluent start schema-registry

[kafka@localhost bin]$ sudo ./confluent start <service-name>

[kafka@localhost confluent-5.1.0]$ bin/connect-standalone etc/schema-registry/connect-avro-standalone.properties etc/kafka-connect-jdbc/source-connector-mysql.properties

kafka@localhost bin]$ sudo ./kafka-topics --list --zookeeper localhost:2181

[kafka@localhost bin]$ sudo ./kafka-topics --list --zookeeper localhost:2181

[kafka@localhost bin]$ sudo ./kafka-avro-console-consumer --topic topic-mysql-account --bootstrap-server localhost:9092 --from-beginning

cassandra

server

[kafka@localhost interface]$ cd ..
[kafka@localhost apache-cassandra-3.11.5]$ cls
bash: cls: command not found...
[kafka@localhost apache-cassandra-3.11.5]$ clear
[kafka@localhost apache-cassandra-3.11.5]$ cd bin/
[kafka@localhost bin]$ ./cassandra 

client

[kafka@localhost bin]$ ./cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.5 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> help

Documented shell commands:
===========================
CAPTURE  CLS          COPY  DESCRIBE  EXPAND  LOGIN   SERIAL  SOURCE   UNICODE
CLEAR    CONSISTENCY  DESC  EXIT      HELP    PAGING  SHOW    TRACING

CQL help topics:
================
AGGREGATES               CREATE_KEYSPACE           DROP_TRIGGER      TEXT     
ALTER_KEYSPACE           CREATE_MATERIALIZED_VIEW  DROP_TYPE         TIME     
ALTER_MATERIALIZED_VIEW  CREATE_ROLE               DROP_USER         TIMESTAMP
ALTER_TABLE              CREATE_TABLE              FUNCTIONS         TRUNCATE 
ALTER_TYPE               CREATE_TRIGGER            GRANT             TYPES    
ALTER_USER               CREATE_TYPE               INSERT            UPDATE   
APPLY                    CREATE_USER               INSERT_JSON       USE      
ASCII                    DATE                      INT               UUID     
BATCH                    DELETE                    JSON            
BEGIN                    DROP_AGGREGATE            KEYWORDS        
BLOB                     DROP_COLUMNFAMILY         LIST_PERMISSIONS
BOOLEAN                  DROP_FUNCTION             LIST_ROLES      
COUNTER                  DROP_INDEX                LIST_USERS      
CREATE_AGGREGATE         DROP_KEYSPACE             PERMISSIONS     
CREATE_COLUMNFAMILY      DROP_MATERIALIZED_VIEW    REVOKE          
CREATE_FUNCTION          DROP_ROLE                 SELECT          
CREATE_INDEX             DROP_TABLE                SELECT_JSON     

keyspace is like a table

cqlsh> create keyspace kafkatraining_keyspace with replication={'class':'SimpleStrategy', 'replication_factor':1};

cqlsh> describe tables;

https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html

http://cassandra.apache.org/download/

https://lenses.io/connect/kafka-to-cassandra/

http://docs.lenses.io/connectors/sink/cassandra.html

http://itechseeker.com/en/tutorials-2/apache-cassandra/connecting-kafka-to-cassandra-sink/

https://drive.google.com/file/d/1xK4yv6Z1kNxHgnZKJFGmRbG34ToP2tht/view?usp=sharing

http://collabedit.com/xhtpe

cqlsh> describe tables;

Keyspace system_schema
----------------------
tables     triggers    views    keyspaces  dropped_columns
functions  aggregates  indexes  types      columns        

Keyspace system_auth
--------------------
resource_role_permissons_index  role_permissions  role_members  roles

Keyspace system
---------------
available_ranges          peers               batchlog        transferred_ranges
batches                   compaction_history  size_estimates  hints             
prepared_statements       sstable_activity    built_views   
"IndexInfo"               peer_events         range_xfers   
views_builds_in_progress  paxos               local         

Keyspace system_distributed
---------------------------
repair_history  view_build_status  parent_repair_history

Keyspace system_traces
----------------------
events  sessions

Keyspace kafkatraining_keyspace
-------------------------------
<empty>

cqlsh> use kafkatraining_keyspace
   ... ;
cqlsh:kafkatraining_keyspace> create table account (account_no int primary_key, customer_id text, acc_type text);
SyntaxException: line 1:37 no viable alternative at input 'primary_key' (create table account (account_no [int] primary_key...)
cqlsh:kafkatraining_keyspace> create table account (account_no int primary key, customer_id text, acc_type text);
cqlsh:kafkatraining_keyspace> show tables;
Improper show command.
cqlsh:kafkatraining_keyspace> describe kafkatraining_keyspace

CREATE KEYSPACE kafkatraining_keyspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

CREATE TABLE kafkatraining_keyspace.account (
    account_no int PRIMARY KEY,
    acc_type text,
    customer_id text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

cqlsh:kafkatraining_keyspace> 

[kafka@localhost confluent-5.1.0]$ bin/connect-standalone etc/schema-registry/connect-avro-standalone.properties etc/kafka-connect-jdbc/source-connector-mysql.properties etc/kafka-connect-cassandra/sink-connect-cassandra.properties

cqlsh:kafkatraining_keyspace> drop table account;
cqlsh:kafkatraining_keyspace> create table account (account_no int primary key, customer_id int, acc_type text);
cqlsh:kafkatraining_keyspace> select * from account;


D:\rajesh\kafka_2.12-2.3.1>bin\windows\connect-standalone.bat config\connect-standalone.properties config\source-connector-mysql.properties config\connect-file-sink.properties

bin\windows\connect-standalone.bat config\connect-standalone.properties config\source-connector-mysql.properties config\connect-file-sink.properties

server-secure.properties

listeners=SASL_PLAINTEXT://localhost:9092
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN

server-jaas-config.properties

KafkaServer {
   org.apache.kafka.common.security.plain.PlainLoginModule required
   username="admin"
   password="admin-secret"
   user_admin="admin-secret"
   user_alice="alice-secret";
};

client-jaas-config.properties

KafkaClient {
  org.apache.kafka.common.security.plain.PlainLoginModule required
  username="alice"
  password="alice-secret";
};

D:\rajesh\kafka_2.12-2.3.1\logs\server.log

listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

TO ENABLE SASL SECURITY

set KAFKA_OPTS=-Djava.security.auth.login.config=config/server-jaas-config.properties

-Djava.security.auth.login.config=client-jaas-config.properties

D:\rajesh\kafka_2.12-2.3.1>set KAFKA_OPTS=-Djava.security.auth.login.config=config/server-jaas-config.properties

D:\rajesh\kafka_2.12-2.3.1>bin\windows\kafka-server-start.bat config\server-secure.properties

D:\rajesh\kafka_2.12-2.3.1>set KAFKA_OPTS=-Djava.security.auth.login.config=config\client-jaas-config.properties

D:\rajesh\kafka_2.12-2.3.1>

bin\windows\kafka-console-consumer.bat --topic first-topic --bootstrap-server localhost:9092 --consumer.config config\consumer-jaas.properties

-Djava.security.auth.login.config=client-jaas-config.properties

D:\rajesh\kafka_2.12-2.3.1>set KAFKA_OPTS=-Djava.security.auth.login.config=config\client-jaas-config.properties

D:\rajesh\kafka_2.12-2.3.1>bin\windows\kafka-console-consumer.bat --topic first-topic --bootstrap-server localhost:9092 --consumer.config config\consumer-jaas.properties

D:\rajesh\kafka_2.12-2.3.1\bin\windows>set KAFKA_OPTS=-Djava.security.auth.login.config=config\consumer-jaas.properties

D:\rajesh\kafka_2.12-2.3.1>bin\windows\kafka-console-consumer.bat --topic first-topic --bootstrap-server localhost:9092 --consumer.config config\consumer-jaas.properties

D:\rajesh\kafka_2.12-2.3.1>set KAFKA_OPTS=-Djava.security.auth.login.config=config\consumer-jaas.properties

D:\rajesh\kafka_2.12-2.3.1>bin\windows\kafka-console-producer.bat --topic first-topic --broker-list localhost:9092 --producer.config config\consumer-jaas.properties

https://kafka.apache.org/documentation/#security_ssl

keytool -keystore server.keystore.jks -alias localhost -validity 365 -genkey -keyalg RSA

kafkassltest

D:\rajesh\kafka-ssl>keytool -keystore server.keystore.jks -alias localhost -validity 365 -genkey -keyalg RSA
Enter keystore password:
Re-enter new password:
What is your first and last name?
  [Unknown]:  rajesh
What is the name of your organizational unit?
  [Unknown]:  training
What is the name of your organization?
  [Unknown]:  rps
What is the name of your City or Locality?
  [Unknown]:  hyderabad
What is the name of your State or Province?
  [Unknown]:  telangana
What is the two-letter country code for this unit?
  [Unknown]:  IN
Is CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN correct?
  [no]:  yes

Enter key password for <localhost>
        (RETURN if same as keystore password):

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore server.keystore.jks -destkeystore server.keystore.jks -deststoretype pkcs12".

D:\rajesh\kafka-ssl>

set PATH=D:\rajesh\openssl-0.9.8k_WIN32\bin;%PATH%

set OPENSSL_CONF=D:\rajesh\openssl-0.9.8k_WIN32\openssl.cnf

openssl req -new -x509 -keyout ca-key -out ca-cert -days 365

opensslpass

CA certificate authority

D:\rajesh\kafka-ssl>set PATH=D:\rajesh\openssl-0.9.8k_WIN32\bin;%PATH%

D:\rajesh\kafka-ssl>set OPENSSL_CONF=D:\rajesh\openssl-0.9.8k_WIN32\openssl.cnf

D:\rajesh\kafka-ssl>echo path
path

D:\rajesh\kafka-ssl>echo %PATH%
D:\rajesh\openssl-0.9.8k_WIN32\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\Java\jdk1.8.0_161\bin;D:\rajesh\apache-maven-3.6.2\bin;D:\rajesh\apache-zookeeper-3.5.6\bin;C:\Program Files\Git\cmd;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;

D:\rajesh\kafka-ssl>openssl req -new -x509 -keyout ca-key -out ca-cert -days 365
Loading 'screen' into random state - done
Generating a 1024 bit RSA private key
........++++++
..................++++++
writing new private key to 'ca-key'
Enter PEM pass phrase:
Verifying - Enter PEM pass phrase:
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:IN
State or Province Name (full name) [Some-State]:telangana
Locality Name (eg, city) []:hyderabad
Organization Name (eg, company) [Internet Widgits Pty Ltd]:rps
Organizational Unit Name (eg, section) []:training
Common Name (eg, YOUR name) []:rajesh
Email Address []:

D:\rajesh\kafka-ssl>

keytool -keystore client.truststore.jks -alias CARoot -import -file ca-cert

kafkassltest

D:\rajesh\kafka-ssl>keytool -keystore client.truststore.jks -alias CARoot -import -file ca-cert
Enter keystore password:
Re-enter new password:
Owner: CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN
Issuer: CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN
Serial number: bae54f92b2500ed5
Valid from: Fri Nov 29 12:09:15 IST 2019 until: Sat Nov 28 12:09:15 IST 2020
Certificate fingerprints:
         MD5:  08:7F:14:9A:5A:23:23:F7:BB:70:35:A1:0C:4A:18:BF
         SHA1: 3F:E7:2E:EB:90:D2:A5:C1:C5:15:61:E4:2A:FB:28:50:62:8B:24:3E
         SHA256: E2:35:43:7A:66:81:22:BE:24:75:19:07:DD:10:70:8E:38:13:98:2D:5B:00:5E:EB:23:12:03:BE:98:08:F7:7D
Signature algorithm name: SHA1withRSA
Subject Public Key Algorithm: 1024-bit RSA key
Version: 3

Extensions:

#1: ObjectId: 2.5.29.35 Criticality=false
AuthorityKeyIdentifier [
KeyIdentifier [
0000: 6B BF 7A 40 42 30 7F D3   47 E3 82 46 B9 E0 DE 46  k.z@B0..G..F...F
0010: EA 44 E3 DD                                        .D..
]
[CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN]
SerialNumber: [    bae54f92 b2500ed5]
]

#2: ObjectId: 2.5.29.19 Criticality=false
BasicConstraints:[
  CA:true
  PathLen:2147483647
]

#3: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: 6B BF 7A 40 42 30 7F D3   47 E3 82 46 B9 E0 DE 46  k.z@B0..G..F...F
0010: EA 44 E3 DD                                        .D..
]
]

Trust this certificate? [no]:  yes
Certificate was added to keystore

D:\rajesh\kafka-ssl>

keytool -keystore server.truststore.jks -alias CARoot -import -file ca-cert

kafkassltest

D:\rajesh\kafka-ssl>keytool -keystore server.truststore.jks -alias CARoot -import -file ca-cert
Enter keystore password:
Re-enter new password:
Owner: CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN
Issuer: CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN
Serial number: bae54f92b2500ed5
Valid from: Fri Nov 29 12:09:15 IST 2019 until: Sat Nov 28 12:09:15 IST 2020
Certificate fingerprints:
         MD5:  08:7F:14:9A:5A:23:23:F7:BB:70:35:A1:0C:4A:18:BF
         SHA1: 3F:E7:2E:EB:90:D2:A5:C1:C5:15:61:E4:2A:FB:28:50:62:8B:24:3E
         SHA256: E2:35:43:7A:66:81:22:BE:24:75:19:07:DD:10:70:8E:38:13:98:2D:5B:00:5E:EB:23:12:03:BE:98:08:F7:7D
Signature algorithm name: SHA1withRSA
Subject Public Key Algorithm: 1024-bit RSA key
Version: 3

Extensions:

#1: ObjectId: 2.5.29.35 Criticality=false
AuthorityKeyIdentifier [
KeyIdentifier [
0000: 6B BF 7A 40 42 30 7F D3   47 E3 82 46 B9 E0 DE 46  k.z@B0..G..F...F
0010: EA 44 E3 DD                                        .D..
]
[CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN]
SerialNumber: [    bae54f92 b2500ed5]
]

#2: ObjectId: 2.5.29.19 Criticality=false
BasicConstraints:[
  CA:true
  PathLen:2147483647
]

#3: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: 6B BF 7A 40 42 30 7F D3   47 E3 82 46 B9 E0 DE 46  k.z@B0..G..F...F
0010: EA 44 E3 DD                                        .D..
]
]

Trust this certificate? [no]:  yes
Certificate was added to keystore

D:\rajesh\kafka-ssl>keytool -keystore server.keystore.jks -alias localhost -certreq -file cert-file

openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password}

openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days 365 -CAcreateserial -passin pass:opensslpass

D:\rajesh\kafka-ssl>keytool -keystore server.keystore.jks -alias localhost -certreq -file cert-file
Enter keystore password:

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore server.keystore.jks -destkeystore server.keystore.jks -deststoretype pkcs12".

D:\rajesh\kafka-ssl>openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days 365 -CAcreateserial -passin pass:opensslpass
Loading 'screen' into random state - done
Signature ok
subject=/C=IN/ST=telangana/L=hyderabad/O=rps/OU=training/CN=rajesh
Getting CA Private Key

D:\rajesh\kafka-ssl>

keytool -keystore server.keystore.jks -alias CARoot -import -file ca-cert
keytool -keystore server.keystore.jks -alias localhost -import -file cert-signed

D:\rajesh\kafka-ssl>keytool -keystore server.keystore.jks -alias CARoot -import -file ca-cert
Enter keystore password:
Owner: CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN
Issuer: CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN
Serial number: bae54f92b2500ed5
Valid from: Fri Nov 29 12:09:15 IST 2019 until: Sat Nov 28 12:09:15 IST 2020
Certificate fingerprints:
         MD5:  08:7F:14:9A:5A:23:23:F7:BB:70:35:A1:0C:4A:18:BF
         SHA1: 3F:E7:2E:EB:90:D2:A5:C1:C5:15:61:E4:2A:FB:28:50:62:8B:24:3E
         SHA256: E2:35:43:7A:66:81:22:BE:24:75:19:07:DD:10:70:8E:38:13:98:2D:5B:00:5E:EB:23:12:03:BE:98:08:F7:7D
Signature algorithm name: SHA1withRSA
Subject Public Key Algorithm: 1024-bit RSA key
Version: 3

Extensions:

#1: ObjectId: 2.5.29.35 Criticality=false
AuthorityKeyIdentifier [
KeyIdentifier [
0000: 6B BF 7A 40 42 30 7F D3   47 E3 82 46 B9 E0 DE 46  k.z@B0..G..F...F
0010: EA 44 E3 DD                                        .D..
]
[CN=rajesh, OU=training, O=rps, L=hyderabad, ST=telangana, C=IN]
SerialNumber: [    bae54f92 b2500ed5]
]

#2: ObjectId: 2.5.29.19 Criticality=false
BasicConstraints:[
  CA:true
  PathLen:2147483647
]

#3: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: 6B BF 7A 40 42 30 7F D3   47 E3 82 46 B9 E0 DE 46  k.z@B0..G..F...F
0010: EA 44 E3 DD                                        .D..
]
]

Trust this certificate? [no]:  yes
Certificate was added to keystore

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore server.keystore.jks -destkeystore server.keystore.jks -deststoretype pkcs12".

D:\rajesh\kafka-ssl>keytool -keystore server.keystore.jks -alias localhost -import -file cert-signed
Enter keystore password:
Certificate reply was installed in keystore

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore server.keystore.jks -destkeystore server.keystore.jks -deststoretype pkcs12".

listeners=PLAINTEXT://localhsot:9092, SSL://localhost:9093

ssl.endpoint.identification.algorithm=
listeners=PLAINTEXT://localhost:9092,SSL://localhost:9093
ssl.keystore.location=c:/kafka_ssl/server.keystore.jks
ssl.keystore.password=kafkassltest
ssl.key.password=kafkassltest
ssl.truststore.location=c:/kafka_ssl/server.truststore.jks
ssl.truststore.password=kafkassltest

server.log

listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
listeners = PLAINTEXT://localhost:9092,SSL://localhost:9093

EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-11-29 12:42:01,341] INFO Awaiting socket connections on localhost:9093. (kafka.network.Acceptor)
[2019-11-29 12:42:01,565] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(localhost,9093,ListenerName(SSL),SSL) (kafka.network.SocketServer)


openssl s_client -debug -connect localhost:9093 -tls1


kafka-console-consumer.bat --topic kafka-ssl-topic --bootstrap-server locahost:9093 --consumer.config D:\rajesh\kafka-ssl-client\kafka-client.properties --from-beginning

kafka-console-consumer.bat --topic kafka-ssl-topic --bootstrap-server locahost:9093 --consumer.config D:\rajesh\kafka-ssl-client\kafka-client.properties --from-beginning

[kafka@localhost bin]$ sudo ./confluent start ksql-server

curl -X POST -H "Content-Type: application/vnd.kafka.avro.v1+json" \
      --data '{"value_schema": "{\"type\": \"record\", \"name\": \"User\", \"fields\": [{\"name\": \"name\", \"type\": \"string\"}]}", "records": [{"value": {"name": "testUser"}}]}' \
      "http://localhost:8082/topics/avrotest"
  {"offsets":[{"partition":0,"offset":0,"error_code":null,"error":null}],"key_schema_id":null,"value_schema_id":21}


curl -X POST -H "Content-Type: application/vnd.kafka.json.v1+json" \
      -H "Accept: application/vnd.kafka.json.v1+json" --data '{"records":[{"value":{"Id":"1", "name":"Rajesh"}}]}' "http://localhost:8082/topics/jsontest"
  {"offsets":[{"partition":0,"offset":0,"error_code":null,"error":null}],"key_schema_id":null,"value_schema_id":null}


curl -X POST -H "Content-Type: application/vnd.kafka.json.v2+json" -H "Accept: application/vnd.kafka.v2+json" --data '{"records":[{"value":{"empId":1002,"name":"Deva","designation":"Developer"}}]}' http://localhost:8082/topics/test-rest-topic

./kafka-topics --create --topic topic1 --partitions 5 --replication-factor 1 --zookeeper localhost:2181

[kafka@localhost bin]$ pwd
/home/kafka/Desktop/Kafka/Old/confluent-5.1.0/bin

[kafka@localhost bin]$ ./kafka-topics --list --zookeeper localhost:2181

[kafka@localhost bin]$ ./ksql
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
                  
                  ===========================================
                  =        _  __ _____  ____  _             =
                  =       | |/ // ____|/ __ \| |            =
                  =       | ' /| (___ | |  | | |            =
                  =       |  <  \___ \| |  | | |            =
                  =       | . \ ____) | |__| | |____        =
                  =       |_|\_\_____/ \___\_\______|       =
                  =                                         =
                  =  Streaming SQL Engine for Apache Kafka® =
                  ===========================================

Copyright 2017-2018 Confluent Inc.

CLI v5.1.0, Server v5.1.0 located at http://localhost:8088

Having trouble? Type 'help' (case-insensitive) for a rundown of how things work!

ksql> create stream test1 (name varchar, country varchar);



drwxrwxrwx.  4 root  root    75 Nov 27 16:13 Kafka
drwx------. 19 kafka kafka 4096 Nov 27 16:18 ..
[kafka@localhost Desktop]$ cd ..
[kafka@localhost ~]$ cd ~/Desktop/
Kafka/            Old Firefox Data/ 
[kafka@localhost ~]$ cd ~/Desktop/
Kafka/            Old Firefox Data/ 
[kafka@localhost ~]$ cd ~/Desktop/
[kafka@localhost Desktop]$ ls -ltra
total 4
drwxr-xr-x.  4 kafka kafka   43 Nov 24 12:12 .
drwx------.  3 kafka kafka   30 Nov 24 12:12 Old Firefox Data
drwxrwxrwx.  4 root  root    75 Nov 27 16:13 Kafka
drwx------. 19 kafka kafka 4096 Nov 27 16:18 ..
[kafka@localhost Desktop]$ cd Kafka/
[kafka@localhost Kafka]$ ls -ltra
total 778784
drwxr-xr-x. 7 kafka kafka        77 Sep  5 01:02 confluent-5.3.1
drwxr-xr-x. 4 kafka kafka        43 Nov 24 12:12 ..
-rw-------. 1 kafka kafka 797471754 Nov 24 12:44 confluent-5.3.1-2.12.tar.gz
drwxrwxrwx. 4 root  root         75 Nov 27 16:13 .
drwxrwxrwx. 3 root  root         64 Nov 27 16:16 Old
[kafka@localhost Kafka]$ cd Old/confluent-5.1.0/
[kafka@localhost confluent-5.1.0]$ ls -ltra
total 12
drwxr-xr-x.  3 kafka kafka   21 Dec 15  2018 lib
drwxr-xr-x.  7 kafka kafka  106 Dec 15  2018 share
drwxr-xr-x. 23 kafka kafka 4096 Dec 15  2018 etc
drwxr-xr-x.  3 kafka kafka 4096 Dec 15  2018 bin
drwxr-xr-x.  2 kafka kafka  179 Dec 15  2018 src
-rw-r--r--.  1 kafka kafka  871 Dec 15  2018 README
drwxr-xr-x.  7 kafka kafka   77 Dec 15  2018 .
drwxrwxrwx.  3 root  root    64 Nov 27 16:16 ..
[kafka@localhost confluent-5.1.0]$ clear

[kafka@localhost confluent-5.1.0]$ ls 
bin  etc  lib  README  share  src
[kafka@localhost confluent-5.1.0]$ pwd
/home/kafka/Desktop/Kafka/Old/confluent-5.1.0
[kafka@localhost confluent-5.1.0]$ 
[kafka@localhost confluent-5.1.0]$ cd bin/
[kafka@localhost bin]$ sudo ./confluent start
[sudo] password for kafka: 
This CLI is intended for development only, not for production
https://docs.confluent.io/current/cli/index.html

Using CONFLUENT_CURRENT: /tmp/confluent.PN3Stqv5
Starting zookeeper
zookeeper is [UP]
Starting kafka
kafka is [UP]
Starting schema-registry
schema-registry is [UP]
Starting kafka-rest
kafka-rest is [UP]
Starting connect
connect is [UP]
Starting ksql-server
ksql-server is [UP]
Starting control-center
control-center is [UP]
[kafka@localhost bin]$ pwd
/home/kafka/Desktop/Kafka/Old/confluent-5.1.0/bin

connection.url="jdbc:mysql://127.0.0.1:3306/sample?verifyServerCertificate=false&useSSL=true&requireSSL=true"

connection.url=jdbc:mysql://127.0.0.1:3306/trainingdb?verifyServerCertificate=false&useSSL=true&requireSSL=true

connection.url=jdbc:mysql://hostname:3306/boakafkatrainingdb?user=root&password=root
mode=incrementing
incrementing.column.name=account_no
topic.prefix=topic-mysql-
table.whitelist=account_tbl

connection.url=jdbc:mysql://hostname:3306/trainingdb
connection.user=root
connection.password=root
mode=incrementing
incrementing.column.name=account_no

https://paste.ubuntu.com/p/c9CPXqVtgy/

connection.url=jdbc:mysql://192.168.20.153:3306/trainingdb

topic-mysql-account


[kafka@localhost bin]$ sudo ./kafka-avro-console-consumer --topic topic-mysql-account --bootstrap-server localhost:9092 --from-beginning


name=cassandra-sink-account
connector.class=com.datamountaineer.streamreactor.connect.cassandra.sink.CassandraSinkConnector
tasks.max=1
topics=topic-mysql-account
connect.cassandra.kcql=INSERT INTO account select account_no,customer_id,account_type as acc_type from topic-mysql-account
connect.cassandra.contact.points=localhost
connect.cassandra.port=9042
connect.cassandra.key.space=kafkatraining_keyspace
connect.cassandra.username=cassandra
connect.cassandra.password=cassandra

#
# Copyright 2018 Confluent Inc.
#
# Licensed under the Confluent Community License; you may not use this file
# except in compliance with the License.  You may obtain a copy of the License at
#
# http://www.confluent.io/confluent-community-license
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OF ANY KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations under the License.
#

# A simple example that copies all tables from a SQLite database. The first few settings are
# required for all connectors: a name, the connector class to run, and the maximum number of
# tasks to create:
name=mysql-source-jdbc-increment-mode
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=1
# The remaining configs are specific to the JDBC source connector. In this example, we connect to a
# SQLite database stored in the file test.db, use and auto-incrementing column called 'id' to
# detect new rows as they are added, and output to topics prefixed with 'test-sqlite-jdbc-', e.g.
# a table called 'users' will be written to the topic 'test-sqlite-jdbc-users'.
connection.url=jdbc:mysql://192.168.20.153:3306/trainingdb
connection.user=root
connection.password=root
mode=incrementing
incrementing.column.name=account_no
topic.prefix=topic-mysql-
table.whitelist=account

http://itechseeker.com/en/tutorials-2/apache-cassandra/connecting-kafka-to-cassandra-sink/

